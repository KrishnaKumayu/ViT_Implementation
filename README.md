# Vision Transformer (ViT) Exploration - Timeline Format

## Overview
This repository tracks my exploration and implementation of Vision Transformer (ViT) models. The journey involves studying fundamental papers, implementing models on datasets, and exploring multi-task learning.

---

## ðŸ“… **Week 1: Understanding Transformers and Vision Transformers**

### ðŸ“Œ **Day 1: Introduction to Transformers**
- Studied the paper **"Attention Is All You Need"**
- ðŸ“„ [Paper Link](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)

### ðŸ“Œ **Day 2: AN IMAGE IS WORTH 16X16 WORDS:
TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE**
- Read **"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"**
- ðŸ“„ [Paper Link](https://arxiv.org/pdf/2010.11929)

### ðŸ“Œ **Day 3-5: ViT Implementation on MNIST & CIFAR-10**
- Implemented **Vision Transformer (ViT)** on the **MNIST dataset**
- Extended the implementation to the **CIFAR-10 dataset**

---

## ðŸ“… **Week 2: Exploring Multi-Task Learning Models**

### ðŸ“Œ **Day 6-7: Multi-Task Learning Exploration**
- Started reading about **multi-task learning models**
- Explored four key research papers on the topic

---

## ðŸ“… **Week 3: Advanced Studies in Multi-Task Learning**

### ðŸ“Œ **Paper 1:** AutoTaskFormer: Searching Vision Transformers for Multi-task Learning
- ðŸ“„ [Paper Link](https://arxiv.org/pdf/2304.08756)

### ðŸ“Œ **Paper 2:** MulT: An End-to-End Multitask Learning Transformer
- ðŸ“„ [Paper Link](https://arxiv.org/pdf/2205.08303)

### ðŸ“Œ **Paper 3:** M3ViT: Mixture-of-Experts Vision Transformer
for Efficient Multi-task Learning
with Model-Accelerator Co-design
- ðŸ“„ [Paper Link](https://proceedings.neurips.cc/paper_files/paper/2022/file/b653f34d576d1790481e3797cb740214-Paper-Conference.pdf)

### ðŸ“Œ **Paper 4:** Polyhistor: Parameter-Efficient Multi-Task
Adaptation for Dense Vision Tasks
- ðŸ“„ [Paper Link](https://proceedings.neurips.cc/paper_files/paper/2022/file/efb02f96766a3b599c76852abf4d42dd-Paper-Conference.pdf)

### ðŸ“Œ **Decision to Implement Paper 1 Approach**
- After reviewing the papers, we decided to start working on the approach suggested in **Paper 1 (Multi-Task Transformers)**

---

## ðŸš€ **Next Steps**
- Implement multi-task learning with transformers
- Explore parameter-sharing techniques in ViT
- Apply ViT-based multi-task learning to real-world datasets


